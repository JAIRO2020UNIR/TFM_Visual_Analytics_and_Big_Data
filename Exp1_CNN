#Script 1


#Scipt 2
#IMPORTAR LIBRERIAS
import os                    # Trabajar con rutas de carpetas y archivos
import cv2                   # Leer y procesar imágenes
import numpy as np           # Arreglos y operaciones numéricas
from sklearn.model_selection import train_test_split  # Separar datos para entrenar
import matplotlib.pyplot as plt  # Para visualizar imágenes
import tensorflow as tf #PARA ENTRENAMIENTO DEL MODELO
from tensorflow.keras import layers, models
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam

# Ruta base del dataset
dataset_path = "/content/drive/MyDrive/Dataset_BUSI_with_GT"

# Clases y sus etiquetas numéricas
categories = ['normal', 'benign', 'malignant']
label_dict = {'normal': 0, 'benign': 1, 'malignant': 2}

# Tamaño deseado para redimensionar las imágenes
IMG_SIZE = 128

# Listas para almacenar datos
images = []
labels = []

# Límite máximo por clase
MAX_PER_CLASS = 150

# Contador por clase
class_counts = {0: 0, 1: 0, 2: 0}

# Cargar imágenes, ignorando las que contienen "_mask" y limitando cantidad por clase
for category in categories:
    folder_path = os.path.join(dataset_path, category)
    class_label = label_dict[category]

    for file in os.listdir(folder_path):
        if "_mask" in file.lower():
            continue  # Ignorar máscaras

        if class_counts[class_label] >= MAX_PER_CLASS:
            continue  # Saltar si ya alcanzamos el límite por clase

        img_path = os.path.join(folder_path, file)
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)

        if img is not None:
            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
            images.append(img)
            labels.append(class_label)
            class_counts[class_label] += 1


from tensorflow.keras.utils import to_categorical

X = np.array(images) / 255.0  # Normalización
X = X.reshape(-1, IMG_SIZE, IMG_SIZE, 1)  # Asegurar formato para CNN
y = to_categorical(labels, num_classes=3)


model = Sequential()

# Primer bloque convolucional
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.3))

# Segundo bloque convolucional
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.3))

# Tercer bloque convolucional
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.4))

# Aplanamos y conectamos
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(3, activation='softmax'))  # 3 clases: normal, benigno, maligno

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

from sklearn.model_selection import train_test_split

# Dividir datos: 80% entrenamiento, 20% prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


from tensorflow.keras.callbacks import EarlyStopping #agregar esta libreria al inicio

# Detener entrenamiento si la validación no mejora por 5 épocas seguidas
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)


history = model.fit(
    X_train, y_train,
    epochs=30,
    batch_size=25,
    validation_split=0.2,
    callbacks=[early_stop]
)
